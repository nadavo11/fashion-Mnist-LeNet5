{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-06-20T09:27:55.658843Z",
     "end_time": "2024-06-20T09:28:02.440567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in c:\\users\\nadav\\appdata\\roaming\\python\\python310\\site-packages (2.17.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\nadav\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\nadav\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard) (1.64.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\nadav\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard) (3.6)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\nadav\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard) (1.24.4)\n",
      "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in c:\\users\\nadav\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\nadav\\anaconda3\\envs\\pythonproject5\\lib\\site-packages (from tensorboard) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\nadav\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\nadav\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\nadav\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\nadav\\appdata\\roaming\\python\\python310\\site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "\n",
    "! pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# README\n",
    "\n",
    "In this notebook, we will implement the LeNet-5 architecture for the fashion-MNIST dataset.\n",
    "\n",
    "## Usage\n",
    "\n",
    "- easy start: run all the notebook cells from the beginning to the end! shuld work out of the box.\n",
    "- to train the model with different hyperparameters, run the cells under the `training the model` section. there is a cell for each hyperparameter choice, you are set to go!\n",
    "- to evaluate the model, run the cells under the `Evaluate your model` section. note: the models should be saved in the `models` directory. you can change the path to the models directory in the `Evaluate your model` section.\n",
    "- to evaluate the model on the pretrained models located in the drive, first upload them, then LOCATE THE CORRECT PATH, then run the cells under the `Evaluate your model` section.\n",
    "-to track the training process, run the tensorboard cell at the end of the notebook. the tensorboard logs are saved in the `runs` directory."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lenet-5 for fashion mnist"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-22T20:49:30.578132Z",
     "end_time": "2024-06-22T20:49:30.596335Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# fasion-MNIST dataset\n",
    "\n",
    "the fasion-MNIST dataset is a dataset of Zalando's article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends fasion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n",
    "\n",
    "## loading the dataset\n",
    "The dataset can be used from `torchvision.datasets` package. The dataset is downloaded from the internet and saved in the `data` directory. The dataset is loaded using the `torch.utils.data.DataLoader` class. The `DataLoader` class is used to load the data in batches. The `DataLoader` class takes the dataset and the batch size as input and provides an iterator to iterate over the dataset in batches.\n",
    "\n",
    "\n",
    "## Transforms\n",
    "### resize:\n",
    " The images in the fasion-MNIST dataset are of size 28x28. So, it makes sense to use the `transforms.Resize` to convert the images to a size that is compatible with the LeNet-5 architecture's input. The LeNet-5 architecture expects images of size 32x32 as input.\n",
    "the original size of the images in the MNIST dataset is 28x28.\n",
    "\n",
    "### ToTensor:\n",
    "The `transforms.ToTensor` converts the images to PyTorch tensors. The images in the fasion-MNIST dataset are of type `PIL.Image.Image`. The `transforms.ToTensor` converts the images to PyTorch tensors of shape `(C, H, W)` where `C` is the number of channels, `H` is the height of the image, and `W` is the width of the image. The images in the fasion-MNIST dataset are grayscale images, so the number of channels is 1. The shape of the images after applying the `transforms.ToTensor` is `(1, 32, 32)`.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "fashion_mnist_train = datasets.FashionMNIST('data', train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor()\n",
    "]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-22T20:49:33.392694Z",
     "end_time": "2024-06-22T20:49:33.554380Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "fashion_mnist_test = datasets.FashionMNIST('data', train=False, download=True, transform=transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor()\n",
    "]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-22T20:49:39.668434Z",
     "end_time": "2024-06-22T20:49:39.735839Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### split the dataset to test and validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split fashion_mnist_train into train and val sets\n",
    "train_indices, val_indices = train_test_split(list(range(len(fashion_mnist_train))), test_size=0.2, random_state=42)\n",
    "\n",
    "train_subset = Subset(fashion_mnist_train, train_indices)\n",
    "val_subset = Subset(fashion_mnist_train, val_indices)\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(fashion_mnist_test, batch_size=batch_size, shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-22T20:49:41.264020Z",
     "end_time": "2024-06-22T20:49:41.305087Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensorboard\n",
    "\n",
    "tensorboard is a visualization tool that can be used to visualize the training process of a deep learning model. The `torch.utils.tensorboard.SummaryWriter` class is used to write the logs to the tensorboard. The `SummaryWriter` class takes the log directory as input. The logs are written to the log directory in the form of event files. The event files can be visualized using the tensorboard web interface.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "#%tensorboard --logdir runs\n",
    "log_dir = os.path.join(os.getcwd(), \"runs\")\n",
    "os.makedirs(log_dir, exist_ok=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-22T20:49:43.209702Z",
     "end_time": "2024-06-22T20:49:43.268111Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize the dataset\n",
    "let's draw 5 random images from the dataset and visualize them. The images are grayscale images of size 32x32. The images are converted to PyTorch tensors of shape `(1, 32, 32)` using the `transforms.ToTensor` transform. The images are then visualized using the `matplotlib.pyplot.imshow` function. we can also decode the labels to see how the images are labeled."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 2000x500 with 5 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get 5 images\n",
    "imgs = [0]*5\n",
    "labels = [0]*5\n",
    "for idx, i in enumerate(np.random.randint(0, len(fashion_mnist_train), 5)):\n",
    "    imgs[idx], labels[idx] = fashion_mnist_train[i]\n",
    "\n",
    "#visualize the 5 image\n",
    "fig, ax = plt.subplots(1, 5, figsize=(20, 5))\n",
    "for i in range(5):\n",
    "    ax[i].imshow(imgs[i].permute(1, 2, 0), cmap='gray')\n",
    "    ax[i].set_title(fashion_mnist_train.classes[labels[i]])\n",
    "    ax[i].axis('off')\n",
    "    #decode the label\n",
    "    label = fashion_mnist_train.classes[labels[i]]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-22T20:49:46.127223Z",
     "end_time": "2024-06-22T20:49:47.397222Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "markdown bold text -"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LeNet-5 Architecture\n",
    "The LeNet-5 architecture is a convolutional neural network architecture proposed by Yann LeCun in 1998. The LeNet-5 architecture consists of 7 layers. The architecture of the LeNet-5 is as follows:\n",
    "\n",
    "#### **input layer:**\n",
    "The input to the LeNet-5 architecture is a grayscale image of size 32x32.\n",
    "\n",
    "#### Convolution & Average Pooling Layers:\n",
    "next, the input image is passed through a convolutional layer followed by an average pooling layer.\n",
    "The convolutional layer has 6 filters of size 5x5.\n",
    "- **kernel size:** 5x5\n",
    "-  **channels:** $1 \\rightarrow 6$\n",
    "- **stride:** 1\n",
    "The average pooling layer has a filter of size 2x2.\n",
    "- **kernel size:** 2x2\n",
    "- **stride:** 2\n",
    "\n",
    "#### 2nd Convolution & Average Pooling Layers:\n",
    "The output from the first average pooling layer is passed through another convolutional layer followed by another average pooling layer.\n",
    "The convolutional layer has 16 filters of size 5x5.\n",
    "- **kernel size:** 5x5\n",
    "- **channels:** $6 \\rightarrow 16$\n",
    "- **stride:** 1\n",
    "average pooling layer has a filter of size 2x2.\n",
    "- **kernel size:** 2x2\n",
    "- **stride:** 2\n",
    "\n",
    "#### convolutional layer:\n",
    "The output from the second average pooling layer is passed through another convolutional layer.\n",
    "- **kernel size:** 5x5\n",
    "- **output channels:** 120\n",
    "- **stride:** 1\n",
    "\n",
    "#### Fully Connected Layers:\n",
    "The output from the last convolutional layer is flattened and passed through 3 fully connected layers:\n",
    "- **1st fully connected layer:** $120 \\rightarrow 84$\n",
    "- **2nd fully connected layer:** $84 \\rightarrow 10$\n",
    "#### Output Layer:\n",
    "The output from the last fully connected layer is passed through a softmax activation function to get the best of 10 class probabilities.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "class Lenet5(nn.Module):\n",
    "    def __init__(self, dropout_prob=0, batch_norm=False):\n",
    "        super(Lenet5, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm2d(6) if batch_norm else nn.Identity()\n",
    "        self.batch_norm2 = nn.BatchNorm2d(16) if batch_norm else nn.Identity()\n",
    "        self.batch_norm3 = nn.BatchNorm2d(120) if batch_norm else nn.Identity()\n",
    "        self.batch_norm_fc1 = nn.BatchNorm1d(84) if batch_norm else nn.Identity()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Tanh(),\n",
    "            self.batch_norm1,\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Tanh(),\n",
    "            self.batch_norm2,\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 120, kernel_size=5, stride=1),\n",
    "            nn.Tanh(),\n",
    "            self.batch_norm3,\n",
    "        )\n",
    "\n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Tanh(),\n",
    "            self.batch_norm_fc1,)\n",
    "\n",
    "        self.fc2 = nn.Linear(84, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        #x = self.softmax(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-22T20:49:54.531807Z",
     "end_time": "2024-06-22T20:49:54.571159Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "let's initialize the model and print the model summary.then let's try it on a single batch of images to see if the model is working correctly."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenet5(\n",
      "  (batch_norm1): Identity()\n",
      "  (batch_norm2): Identity()\n",
      "  (batch_norm3): Identity()\n",
      "  (batch_norm_fc1): Identity()\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (3): Tanh()\n",
      "    (4): Identity()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (3): Tanh()\n",
      "    (4): Identity()\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (fc2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ")\n",
      "tensor([[-0.1861, -0.0308,  0.1992, -0.1237, -0.0709, -0.0563, -0.0628,  0.0377,\n",
      "         -0.0273,  0.1176]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = Lenet5()\n",
    "print(model)\n",
    "\n",
    "pred = model(fashion_mnist_train[0][0].unsqueeze(0))\n",
    "print (pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-22T20:50:00.889652Z",
     "end_time": "2024-06-22T20:50:01.215998Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## visualizing the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "tb_writer = SummaryWriter('runs/lenet5')\n",
    "\n",
    "# visualize the model in our tensorboard summary\n",
    "tb_writer.add_graph(model, fashion_mnist_train[0][0].unsqueeze(0))\n",
    "tb_writer.flush()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-22T20:50:05.893583Z",
     "end_time": "2024-06-22T20:50:06.481428Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 256\n",
    "dropout_prob = 0.3\n",
    "weight_decay = 0.0001\n",
    "batch_norm = False\n",
    "learning_rate= 1e-3\n",
    "epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# training the model\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(fashion_mnist_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-22T22:27:21.302269Z",
     "end_time": "2024-06-22T22:27:21.351729Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# training loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    \"\"\"\n",
    "    evaluate the model on the validation set\n",
    "    :param model: model to evaluate\n",
    "    :param val_loader: validation dataset loader\n",
    "    :return: (accuracy, loss)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            running_acc += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        accuracy = running_acc / total\n",
    "        loss = running_loss/ len(val_loader)\n",
    "    return accuracy, loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-22T22:27:37.875648Z",
     "end_time": "2024-06-22T22:27:37.903146Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def train(model = Lenet5,\n",
    "          train_loader=train_loader,\n",
    "          val_loader=val_loader,\n",
    "          criterion = criterion,\n",
    "          optimizer = optimizer,\n",
    "          epochs=epochs,\n",
    "          session = None,\n",
    "          ):\n",
    "    #create /models directory IF it does not exist\n",
    "    if not os.path.exists('/models'):\n",
    "        # Create the directory\n",
    "        os.makedirs('/models')\n",
    "\n",
    "    # track with tensorboard\n",
    "    session = session or 'Lenet5'+datetime.now().strftime('%m-%d-%H-%M')\n",
    "    #tb_writer = SummaryWriter(f'runs/{session}')\n",
    "    run_dir = f'{log_dir}/{session}'\n",
    "    print(run_dir)\n",
    "    tb_writer = SummaryWriter(run_dir)\n",
    "    tb_writer.flush()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        train_total = 0\n",
    "        val_loss_min = np.Inf\n",
    "        # set the model to train mode\n",
    "        model.train(True)\n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "            # get the input image and labels\n",
    "            inputs, labels = data\n",
    "\n",
    "            # start with zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            outputs = model(inputs)\n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            if i % 100 == 99:\n",
    "                # print gradient statistics\n",
    "                for name, param in model.named_parameters():\n",
    "                    tb_writer.add_histogram(name, param.grad, epoch * len(train_loader) +  i)\n",
    "                # loss of current batch\n",
    "                avg_train_loss = running_loss / 100\n",
    "                tb_writer.add_scalar('training loss', avg_train_loss, epoch * len(train_loader) +  i)\n",
    "\n",
    "                #print(f'[{epoch + 1}, {i + 1}] loss: {avg_train_loss}')\n",
    "                running_loss = 0.0\n",
    "        \"\"\"-----------------\n",
    "        per epoch evaluation\n",
    "        -----------------\"\"\"\n",
    "        # set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        #accuracy\n",
    "        train_accuracy = running_corrects / train_total\n",
    "        # validation\n",
    "        val_acc, val_loss = evaluate(model, val_loader)\n",
    "\n",
    "        #val_accuracy = (val_preds == val_labels).sum().item() / len(val_loader)\n",
    "        tb_writer.add_scalars('train vs val loss', {'train': avg_train_loss, 'val': val_loss}, epoch)\n",
    "        tb_writer.add_scalars('train vs val accuracy', {'train': train_accuracy, 'val': val_acc}, epoch)\n",
    "        print(f'at epoch {epoch}: \\nvalidation loss: {val_loss} \\ntraining loss:   {avg_train_loss} ')\n",
    "        tb_writer.add_scalar('validation loss', val_loss, epoch)\n",
    "        if val_loss <= val_loss_min:\n",
    "            print('validation loss decreased({:.6f} -->{:.6f}). Saving Model ...'.format(val_loss_min, val_loss))\n",
    "            torch.save(model, f'./models/Lenet {session}.pt')\n",
    "            val_loss_min = val_loss\n",
    "    print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-27T20:48:06.919281Z",
     "end_time": "2024-06-27T20:48:07.096060Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameters\n",
    "Let's investigate the results for different hyperparameters choices!\n",
    "tensorboard allows us to compare the results of different hyperparameters choices. We can compare the results of different hyperparameters choices using the tensorboard web interface. We can compare the training and validation loss for different hyperparameters choices using the tensorboard web interface."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## training the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nadav\\DataspellProjects\\Lenet5\\runs/vanilla\n",
      "at epoch 0: \n",
      "validation loss: 0.6224057173475306 \n",
      "training loss:   1.1512283116579056 \n",
      "validation loss decreased(inf -->0.622406). Saving Model ...\n",
      "at epoch 1: \n",
      "validation loss: 0.5075520226295959 \n",
      "training loss:   0.5833996060490608 \n",
      "validation loss decreased(inf -->0.507552). Saving Model ...\n",
      "at epoch 2: \n",
      "validation loss: 0.4567886451457409 \n",
      "training loss:   0.48321772992610934 \n",
      "validation loss decreased(inf -->0.456789). Saving Model ...\n",
      "at epoch 3: \n",
      "validation loss: 0.42565467763454357 \n",
      "training loss:   0.4387798571586609 \n",
      "validation loss decreased(inf -->0.425655). Saving Model ...\n",
      "at epoch 4: \n",
      "validation loss: 0.39451743947698714 \n",
      "training loss:   0.41017566829919816 \n",
      "validation loss decreased(inf -->0.394517). Saving Model ...\n",
      "at epoch 5: \n",
      "validation loss: 0.39108320056123935 \n",
      "training loss:   0.38246782928705214 \n",
      "validation loss decreased(inf -->0.391083). Saving Model ...\n",
      "at epoch 6: \n",
      "validation loss: 0.36479066787882053 \n",
      "training loss:   0.36054535701870916 \n",
      "validation loss decreased(inf -->0.364791). Saving Model ...\n",
      "at epoch 7: \n",
      "validation loss: 0.359057200081805 \n",
      "training loss:   0.34734899014234544 \n",
      "validation loss decreased(inf -->0.359057). Saving Model ...\n",
      "at epoch 8: \n",
      "validation loss: 0.340716467258778 \n",
      "training loss:   0.32999961465597155 \n",
      "validation loss decreased(inf -->0.340716). Saving Model ...\n",
      "at epoch 9: \n",
      "validation loss: 0.33107794282284186 \n",
      "training loss:   0.31763848170638087 \n",
      "validation loss decreased(inf -->0.331078). Saving Model ...\n",
      "at epoch 10: \n",
      "validation loss: 0.3325892132013402 \n",
      "training loss:   0.31571948394179344 \n",
      "validation loss decreased(inf -->0.332589). Saving Model ...\n",
      "at epoch 11: \n",
      "validation loss: 0.3345801465688868 \n",
      "training loss:   0.2919864408671856 \n",
      "validation loss decreased(inf -->0.334580). Saving Model ...\n",
      "at epoch 12: \n",
      "validation loss: 0.3299321348362781 \n",
      "training loss:   0.28722965210676193 \n",
      "validation loss decreased(inf -->0.329932). Saving Model ...\n",
      "at epoch 13: \n",
      "validation loss: 0.32226061567347103 \n",
      "training loss:   0.28377138793468476 \n",
      "validation loss decreased(inf -->0.322261). Saving Model ...\n",
      "at epoch 14: \n",
      "validation loss: 0.3157135200627307 \n",
      "training loss:   0.27096024587750434 \n",
      "validation loss decreased(inf -->0.315714). Saving Model ...\n",
      "at epoch 15: \n",
      "validation loss: 0.3060278169652249 \n",
      "training loss:   0.26692829206585883 \n",
      "validation loss decreased(inf -->0.306028). Saving Model ...\n",
      "at epoch 16: \n",
      "validation loss: 0.31510481270069773 \n",
      "training loss:   0.25874747678637505 \n",
      "validation loss decreased(inf -->0.315105). Saving Model ...\n",
      "at epoch 17: \n",
      "validation loss: 0.3202450668558161 \n",
      "training loss:   0.25178378731012346 \n",
      "validation loss decreased(inf -->0.320245). Saving Model ...\n",
      "at epoch 18: \n",
      "validation loss: 0.30969199316298707 \n",
      "training loss:   0.25017702013254167 \n",
      "validation loss decreased(inf -->0.309692). Saving Model ...\n",
      "at epoch 19: \n",
      "validation loss: 0.3075560359878743 \n",
      "training loss:   0.2371201877295971 \n",
      "validation loss decreased(inf -->0.307556). Saving Model ...\n",
      "at epoch 20: \n",
      "validation loss: 0.3031999319791794 \n",
      "training loss:   0.23273143887519837 \n",
      "validation loss decreased(inf -->0.303200). Saving Model ...\n",
      "at epoch 21: \n",
      "validation loss: 0.30543069541454315 \n",
      "training loss:   0.22917566046118737 \n",
      "validation loss decreased(inf -->0.305431). Saving Model ...\n",
      "at epoch 22: \n",
      "validation loss: 0.30198572639455185 \n",
      "training loss:   0.21794415667653083 \n",
      "validation loss decreased(inf -->0.301986). Saving Model ...\n",
      "at epoch 23: \n",
      "validation loss: 0.29818154903168376 \n",
      "training loss:   0.21866343319416046 \n",
      "validation loss decreased(inf -->0.298182). Saving Model ...\n",
      "at epoch 24: \n",
      "validation loss: 0.3022939851309391 \n",
      "training loss:   0.21386618986725808 \n",
      "validation loss decreased(inf -->0.302294). Saving Model ...\n",
      "at epoch 25: \n",
      "validation loss: 0.2957773018390574 \n",
      "training loss:   0.21104187682271003 \n",
      "validation loss decreased(inf -->0.295777). Saving Model ...\n",
      "at epoch 26: \n",
      "validation loss: 0.30143169424635297 \n",
      "training loss:   0.20423975437879563 \n",
      "validation loss decreased(inf -->0.301432). Saving Model ...\n",
      "at epoch 27: \n",
      "validation loss: 0.3008092524523431 \n",
      "training loss:   0.20037544786930084 \n",
      "validation loss decreased(inf -->0.300809). Saving Model ...\n",
      "at epoch 28: \n",
      "validation loss: 0.30572285075136957 \n",
      "training loss:   0.19333277329802512 \n",
      "validation loss decreased(inf -->0.305723). Saving Model ...\n",
      "at epoch 29: \n",
      "validation loss: 0.31834729079236374 \n",
      "training loss:   0.18987823456525801 \n",
      "validation loss decreased(inf -->0.318347). Saving Model ...\n",
      "at epoch 30: \n",
      "validation loss: 0.30430074828736325 \n",
      "training loss:   0.18701556123793125 \n",
      "validation loss decreased(inf -->0.304301). Saving Model ...\n",
      "at epoch 31: \n",
      "validation loss: 0.3142799471921109 \n",
      "training loss:   0.18001981429755687 \n",
      "validation loss decreased(inf -->0.314280). Saving Model ...\n",
      "at epoch 32: \n",
      "validation loss: 0.30633609028572734 \n",
      "training loss:   0.18064176827669143 \n",
      "validation loss decreased(inf -->0.306336). Saving Model ...\n",
      "at epoch 33: \n",
      "validation loss: 0.3079397253533627 \n",
      "training loss:   0.17125048354268074 \n",
      "validation loss decreased(inf -->0.307940). Saving Model ...\n",
      "at epoch 34: \n",
      "validation loss: 0.3078336043560759 \n",
      "training loss:   0.16856556594371797 \n",
      "validation loss decreased(inf -->0.307834). Saving Model ...\n",
      "at epoch 35: \n",
      "validation loss: 0.3031557325987106 \n",
      "training loss:   0.16449835941195487 \n",
      "validation loss decreased(inf -->0.303156). Saving Model ...\n",
      "at epoch 36: \n",
      "validation loss: 0.31667562463182086 \n",
      "training loss:   0.16213047549128531 \n",
      "validation loss decreased(inf -->0.316676). Saving Model ...\n",
      "at epoch 37: \n",
      "validation loss: 0.3130573478150875 \n",
      "training loss:   0.15498259231448175 \n",
      "validation loss decreased(inf -->0.313057). Saving Model ...\n",
      "at epoch 38: \n",
      "validation loss: 0.31204287327350455 \n",
      "training loss:   0.15309307485818863 \n",
      "validation loss decreased(inf -->0.312043). Saving Model ...\n",
      "at epoch 39: \n",
      "validation loss: 0.310467022530576 \n",
      "training loss:   0.15036905348300933 \n",
      "validation loss decreased(inf -->0.310467). Saving Model ...\n",
      "at epoch 40: \n",
      "validation loss: 0.3258264071129738 \n",
      "training loss:   0.14586606457829476 \n",
      "validation loss decreased(inf -->0.325826). Saving Model ...\n",
      "at epoch 41: \n",
      "validation loss: 0.3164926770519703 \n",
      "training loss:   0.1420410630851984 \n",
      "validation loss decreased(inf -->0.316493). Saving Model ...\n",
      "at epoch 42: \n",
      "validation loss: 0.3198827156361113 \n",
      "training loss:   0.13921924650669099 \n",
      "validation loss decreased(inf -->0.319883). Saving Model ...\n",
      "at epoch 43: \n",
      "validation loss: 0.3224540777662967 \n",
      "training loss:   0.1321254964917898 \n",
      "validation loss decreased(inf -->0.322454). Saving Model ...\n",
      "at epoch 44: \n",
      "validation loss: 0.3223999627727143 \n",
      "training loss:   0.12764699194580317 \n",
      "validation loss decreased(inf -->0.322400). Saving Model ...\n",
      "at epoch 45: \n",
      "validation loss: 0.32277646565690954 \n",
      "training loss:   0.12427447102963925 \n",
      "validation loss decreased(inf -->0.322776). Saving Model ...\n",
      "at epoch 46: \n",
      "validation loss: 0.33202521217630265 \n",
      "training loss:   0.12171532113105059 \n",
      "validation loss decreased(inf -->0.332025). Saving Model ...\n",
      "at epoch 47: \n",
      "validation loss: 0.33559600882073665 \n",
      "training loss:   0.11920172668993473 \n",
      "validation loss decreased(inf -->0.335596). Saving Model ...\n",
      "at epoch 48: \n",
      "validation loss: 0.34612734235347586 \n",
      "training loss:   0.11420357413589954 \n",
      "validation loss decreased(inf -->0.346127). Saving Model ...\n",
      "at epoch 49: \n",
      "validation loss: 0.3431222850972034 \n",
      "training loss:   0.1150893235206604 \n",
      "validation loss decreased(inf -->0.343122). Saving Model ...\n",
      "at epoch 50: \n",
      "validation loss: 0.35748051741021747 \n",
      "training loss:   0.1064469875767827 \n",
      "validation loss decreased(inf -->0.357481). Saving Model ...\n",
      "at epoch 51: \n",
      "validation loss: 0.35268866571974244 \n",
      "training loss:   0.10245193589478731 \n",
      "validation loss decreased(inf -->0.352689). Saving Model ...\n",
      "at epoch 52: \n",
      "validation loss: 0.36430412974763426 \n",
      "training loss:   0.10155058924108744 \n",
      "validation loss decreased(inf -->0.364304). Saving Model ...\n",
      "at epoch 53: \n",
      "validation loss: 0.36149701507801707 \n",
      "training loss:   0.10069711364805699 \n",
      "validation loss decreased(inf -->0.361497). Saving Model ...\n",
      "at epoch 54: \n",
      "validation loss: 0.3642811283786246 \n",
      "training loss:   0.09438864603638648 \n",
      "validation loss decreased(inf -->0.364281). Saving Model ...\n",
      "at epoch 55: \n",
      "validation loss: 0.3733432353176969 \n",
      "training loss:   0.09409942097961903 \n",
      "validation loss decreased(inf -->0.373343). Saving Model ...\n",
      "at epoch 56: \n",
      "validation loss: 0.370800828680079 \n",
      "training loss:   0.08884191241115331 \n",
      "validation loss decreased(inf -->0.370801). Saving Model ...\n",
      "at epoch 57: \n",
      "validation loss: 0.38078324052881685 \n",
      "training loss:   0.08320320636034012 \n",
      "validation loss decreased(inf -->0.380783). Saving Model ...\n",
      "at epoch 58: \n",
      "validation loss: 0.3735080684753174 \n",
      "training loss:   0.08242280926555395 \n",
      "validation loss decreased(inf -->0.373508). Saving Model ...\n",
      "at epoch 59: \n",
      "validation loss: 0.3923196187044712 \n",
      "training loss:   0.0767261279374361 \n",
      "validation loss decreased(inf -->0.392320). Saving Model ...\n",
      "at epoch 60: \n",
      "validation loss: 0.3940548858744033 \n",
      "training loss:   0.07647251900285483 \n",
      "validation loss decreased(inf -->0.394055). Saving Model ...\n",
      "at epoch 61: \n",
      "validation loss: 0.39748894787849265 \n",
      "training loss:   0.07921660892665386 \n",
      "validation loss decreased(inf -->0.397489). Saving Model ...\n",
      "at epoch 62: \n",
      "validation loss: 0.4199539524443606 \n",
      "training loss:   0.07191305696964263 \n",
      "validation loss decreased(inf -->0.419954). Saving Model ...\n",
      "at epoch 63: \n",
      "validation loss: 0.4056986155028039 \n",
      "training loss:   0.07190011573955417 \n",
      "validation loss decreased(inf -->0.405699). Saving Model ...\n",
      "at epoch 64: \n",
      "validation loss: 0.4113464891276461 \n",
      "training loss:   0.07096597064286471 \n",
      "validation loss decreased(inf -->0.411346). Saving Model ...\n",
      "at epoch 65: \n",
      "validation loss: 0.422718160964073 \n",
      "training loss:   0.066846558842808 \n",
      "validation loss decreased(inf -->0.422718). Saving Model ...\n",
      "at epoch 66: \n",
      "validation loss: 0.41949544688488577 \n",
      "training loss:   0.06385327337309718 \n",
      "validation loss decreased(inf -->0.419495). Saving Model ...\n",
      "at epoch 67: \n",
      "validation loss: 0.442991445039181 \n",
      "training loss:   0.058432342279702426 \n",
      "validation loss decreased(inf -->0.442991). Saving Model ...\n",
      "at epoch 68: \n",
      "validation loss: 0.436167041037945 \n",
      "training loss:   0.06114233667030931 \n",
      "validation loss decreased(inf -->0.436167). Saving Model ...\n",
      "at epoch 69: \n",
      "validation loss: 0.4430178296058736 \n",
      "training loss:   0.05429601280018687 \n",
      "validation loss decreased(inf -->0.443018). Saving Model ...\n",
      "at epoch 70: \n",
      "validation loss: 0.45161493534737446 \n",
      "training loss:   0.05918179500848055 \n",
      "validation loss decreased(inf -->0.451615). Saving Model ...\n",
      "at epoch 71: \n",
      "validation loss: 0.44430418788118564 \n",
      "training loss:   0.053462086785584687 \n",
      "validation loss decreased(inf -->0.444304). Saving Model ...\n",
      "at epoch 72: \n",
      "validation loss: 0.45793482280792075 \n",
      "training loss:   0.04662701336666942 \n",
      "validation loss decreased(inf -->0.457935). Saving Model ...\n",
      "at epoch 73: \n",
      "validation loss: 0.45277736922527884 \n",
      "training loss:   0.045763459261506795 \n",
      "validation loss decreased(inf -->0.452777). Saving Model ...\n",
      "at epoch 74: \n",
      "validation loss: 0.4737782808060342 \n",
      "training loss:   0.04393939107656479 \n",
      "validation loss decreased(inf -->0.473778). Saving Model ...\n",
      "at epoch 75: \n",
      "validation loss: 0.4765976413767389 \n",
      "training loss:   0.04565675849094987 \n",
      "validation loss decreased(inf -->0.476598). Saving Model ...\n",
      "at epoch 76: \n",
      "validation loss: 0.4866824559074767 \n",
      "training loss:   0.03985965352505445 \n",
      "validation loss decreased(inf -->0.486682). Saving Model ...\n",
      "at epoch 77: \n",
      "validation loss: 0.48271909926800016 \n",
      "training loss:   0.03985753289423883 \n",
      "validation loss decreased(inf -->0.482719). Saving Model ...\n",
      "at epoch 78: \n",
      "validation loss: 0.49209478877960366 \n",
      "training loss:   0.046897482722997666 \n",
      "validation loss decreased(inf -->0.492095). Saving Model ...\n",
      "at epoch 79: \n",
      "validation loss: 0.505004643125737 \n",
      "training loss:   0.04037025148048997 \n",
      "validation loss decreased(inf -->0.505005). Saving Model ...\n",
      "at epoch 80: \n",
      "validation loss: 0.5070477356301978 \n",
      "training loss:   0.03550963239744306 \n",
      "validation loss decreased(inf -->0.507048). Saving Model ...\n",
      "at epoch 81: \n",
      "validation loss: 0.5108526425158724 \n",
      "training loss:   0.03262539020739496 \n",
      "validation loss decreased(inf -->0.510853). Saving Model ...\n",
      "at epoch 82: \n",
      "validation loss: 0.5185236930847168 \n",
      "training loss:   0.035357385110110044 \n",
      "validation loss decreased(inf -->0.518524). Saving Model ...\n",
      "at epoch 83: \n",
      "validation loss: 0.5336813736469188 \n",
      "training loss:   0.03299578249454498 \n",
      "validation loss decreased(inf -->0.533681). Saving Model ...\n",
      "at epoch 84: \n",
      "validation loss: 0.5251342286454871 \n",
      "training loss:   0.03750395583920181 \n",
      "validation loss decreased(inf -->0.525134). Saving Model ...\n",
      "at epoch 85: \n",
      "validation loss: 0.5265649721977559 \n",
      "training loss:   0.02692801524884999 \n",
      "validation loss decreased(inf -->0.526565). Saving Model ...\n",
      "at epoch 86: \n",
      "validation loss: 0.5503238128854874 \n",
      "training loss:   0.027618033587932588 \n",
      "validation loss decreased(inf -->0.550324). Saving Model ...\n",
      "at epoch 87: \n",
      "validation loss: 0.5436812214394833 \n",
      "training loss:   0.02695584480650723 \n",
      "validation loss decreased(inf -->0.543681). Saving Model ...\n",
      "at epoch 88: \n",
      "validation loss: 0.5479552929705762 \n",
      "training loss:   0.029679099582135677 \n",
      "validation loss decreased(inf -->0.547955). Saving Model ...\n",
      "at epoch 89: \n",
      "validation loss: 0.5491352664663437 \n",
      "training loss:   0.026537681566551327 \n",
      "validation loss decreased(inf -->0.549135). Saving Model ...\n",
      "at epoch 90: \n",
      "validation loss: 0.5712816328444379 \n",
      "training loss:   0.0203573826700449 \n",
      "validation loss decreased(inf -->0.571282). Saving Model ...\n",
      "at epoch 91: \n",
      "validation loss: 0.5650913106634262 \n",
      "training loss:   0.025558524094522 \n",
      "validation loss decreased(inf -->0.565091). Saving Model ...\n",
      "at epoch 92: \n",
      "validation loss: 0.5773295893314037 \n",
      "training loss:   0.021497031450271608 \n",
      "validation loss decreased(inf -->0.577330). Saving Model ...\n",
      "at epoch 93: \n",
      "validation loss: 0.5856764012194694 \n",
      "training loss:   0.02166076183784753 \n",
      "validation loss decreased(inf -->0.585676). Saving Model ...\n",
      "at epoch 94: \n",
      "validation loss: 0.5865868856298163 \n",
      "training loss:   0.0225520899053663 \n",
      "validation loss decreased(inf -->0.586587). Saving Model ...\n",
      "at epoch 95: \n",
      "validation loss: 0.5859507455470714 \n",
      "training loss:   0.023583477777428925 \n",
      "validation loss decreased(inf -->0.585951). Saving Model ...\n",
      "at epoch 96: \n",
      "validation loss: 0.6081499317859081 \n",
      "training loss:   0.019112200275994837 \n",
      "validation loss decreased(inf -->0.608150). Saving Model ...\n",
      "at epoch 97: \n",
      "validation loss: 0.5985699879362228 \n",
      "training loss:   0.02422573646530509 \n",
      "validation loss decreased(inf -->0.598570). Saving Model ...\n",
      "at epoch 98: \n",
      "validation loss: 0.6222739815711975 \n",
      "training loss:   0.015146449166350067 \n",
      "validation loss decreased(inf -->0.622274). Saving Model ...\n",
      "at epoch 99: \n",
      "validation loss: 0.6308770357294285 \n",
      "training loss:   0.020272417427040636 \n",
      "validation loss decreased(inf -->0.630877). Saving Model ...\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "learning_rate= 1e-3\n",
    "epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = Lenet5(dropout_prob=0, batch_norm=False)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "train(model = model,\n",
    "      val_loader=val_loader,\n",
    "      train_loader = train_loader,\n",
    "      criterion = criterion,\n",
    "      optimizer = optimizer,\n",
    "      epochs = epochs,\n",
    "      session='vanilla')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-24T22:04:37.302289Z",
     "end_time": "2024-06-24T22:32:05.653129Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# training the model with batch normalization\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nadav\\DataspellProjects\\Lenet5\\runs/with batch norm\n",
      "at epoch 0: \n",
      "validation loss: 0.4112513857953092 \n",
      "training loss:   0.6471644857525826 \n",
      "at epoch 1: \n",
      "validation loss: 0.35254365078946376 \n",
      "training loss:   0.3865198284387589 \n",
      "at epoch 2: \n",
      "validation loss: 0.3368966798198984 \n",
      "training loss:   0.32571368336677553 \n",
      "at epoch 3: \n",
      "validation loss: 0.31358846673306 \n",
      "training loss:   0.2897303807735443 \n",
      "at epoch 4: \n",
      "validation loss: 0.29115065773750876 \n",
      "training loss:   0.2671985977888107 \n",
      "at epoch 5: \n",
      "validation loss: 0.2998339055066413 \n",
      "training loss:   0.24937561333179473 \n",
      "at epoch 6: \n",
      "validation loss: 0.2930199535603219 \n",
      "training loss:   0.23631596118211745 \n",
      "at epoch 7: \n",
      "validation loss: 0.300390103712995 \n",
      "training loss:   0.21889858528971673 \n",
      "at epoch 8: \n",
      "validation loss: 0.2879348482857359 \n",
      "training loss:   0.20530740559101104 \n",
      "at epoch 9: \n",
      "validation loss: 0.2905998965527149 \n",
      "training loss:   0.19537045553326607 \n",
      "at epoch 10: \n",
      "validation loss: 0.30096880394093534 \n",
      "training loss:   0.18645194880664348 \n",
      "at epoch 11: \n",
      "validation loss: 0.30161979540865475 \n",
      "training loss:   0.17163296334445477 \n",
      "at epoch 12: \n",
      "validation loss: 0.306102694983178 \n",
      "training loss:   0.1595696959644556 \n",
      "at epoch 13: \n",
      "validation loss: 0.3152924769736351 \n",
      "training loss:   0.14818365134298803 \n",
      "at epoch 14: \n",
      "validation loss: 0.3198765975997803 \n",
      "training loss:   0.13996310137212276 \n",
      "at epoch 15: \n",
      "validation loss: 0.3285510540008545 \n",
      "training loss:   0.13034224390983581 \n",
      "at epoch 16: \n",
      "validation loss: 0.3221128056658075 \n",
      "training loss:   0.12232757233083248 \n",
      "at epoch 17: \n",
      "validation loss: 0.3588017187219985 \n",
      "training loss:   0.11805151969194412 \n",
      "at epoch 18: \n",
      "validation loss: 0.35080221168538356 \n",
      "training loss:   0.10936973571777343 \n",
      "at epoch 19: \n",
      "validation loss: 0.3553077220282656 \n",
      "training loss:   0.10204342726618051 \n",
      "at epoch 20: \n",
      "validation loss: 0.3658579534038584 \n",
      "training loss:   0.09170553773641586 \n",
      "at epoch 21: \n",
      "validation loss: 0.37667685747146606 \n",
      "training loss:   0.08724227560684085 \n",
      "at epoch 22: \n",
      "validation loss: 0.41112527187834397 \n",
      "training loss:   0.07997041210532188 \n",
      "at epoch 23: \n",
      "validation loss: 0.40674925230919046 \n",
      "training loss:   0.08255439411848783 \n",
      "at epoch 24: \n",
      "validation loss: 0.41067813653895197 \n",
      "training loss:   0.07509324379265309 \n",
      "at epoch 25: \n",
      "validation loss: 0.4206539899744886 \n",
      "training loss:   0.06630409844219684 \n",
      "at epoch 26: \n",
      "validation loss: 0.44523032675398155 \n",
      "training loss:   0.05925265064463019 \n",
      "at epoch 27: \n",
      "validation loss: 0.44946372509002686 \n",
      "training loss:   0.05977621093392372 \n",
      "at epoch 28: \n",
      "validation loss: 0.482046384760674 \n",
      "training loss:   0.0582884987257421 \n",
      "at epoch 29: \n",
      "validation loss: 0.4710065250067001 \n",
      "training loss:   0.055579956714063884 \n",
      "at epoch 30: \n",
      "validation loss: 0.4882097491558562 \n",
      "training loss:   0.05398117395117879 \n",
      "at epoch 31: \n",
      "validation loss: 0.4940642711329967 \n",
      "training loss:   0.04470382331870496 \n",
      "at epoch 32: \n",
      "validation loss: 0.5156149521787116 \n",
      "training loss:   0.03893829582259059 \n",
      "at epoch 33: \n",
      "validation loss: 0.5200573256675233 \n",
      "training loss:   0.04114145703613758 \n",
      "at epoch 34: \n",
      "validation loss: 0.5389494293547691 \n",
      "training loss:   0.03861473275348544 \n",
      "at epoch 35: \n",
      "validation loss: 0.5417877740048348 \n",
      "training loss:   0.036769678285345435 \n",
      "at epoch 36: \n",
      "validation loss: 0.5672735235792525 \n",
      "training loss:   0.03292336503509432 \n",
      "at epoch 37: \n",
      "validation loss: 0.5842870708475721 \n",
      "training loss:   0.0379117096401751 \n",
      "at epoch 38: \n",
      "validation loss: 0.6051312634285461 \n",
      "training loss:   0.03330976628698409 \n",
      "at epoch 39: \n",
      "validation loss: 0.601621963876359 \n",
      "training loss:   0.03683353893458843 \n",
      "at epoch 40: \n",
      "validation loss: 0.5778526644757453 \n",
      "training loss:   0.037696785666048525 \n",
      "at epoch 41: \n",
      "validation loss: 0.6254821172420014 \n",
      "training loss:   0.02430373538285494 \n",
      "at epoch 42: \n",
      "validation loss: 0.6069901312919374 \n",
      "training loss:   0.023296573660336436 \n",
      "at epoch 43: \n",
      "validation loss: 0.6299356260198228 \n",
      "training loss:   0.024393927592318506 \n",
      "at epoch 44: \n",
      "validation loss: 0.6414053351321118 \n",
      "training loss:   0.028235577670857312 \n",
      "at epoch 45: \n",
      "validation loss: 0.6190417244079265 \n",
      "training loss:   0.029789676126092672 \n",
      "at epoch 46: \n",
      "validation loss: 0.6303521794207553 \n",
      "training loss:   0.03346105048898607 \n",
      "at epoch 47: \n",
      "validation loss: 0.6459764210467643 \n",
      "training loss:   0.021540642492473127 \n",
      "at epoch 48: \n",
      "validation loss: 0.6525697467174936 \n",
      "training loss:   0.020906071313656866 \n",
      "at epoch 49: \n",
      "validation loss: 0.6538690905621711 \n",
      "training loss:   0.024457463589496912 \n",
      "at epoch 50: \n",
      "validation loss: 0.6528162341168586 \n",
      "training loss:   0.022858294602483512 \n",
      "at epoch 51: \n",
      "validation loss: 0.6620457216780237 \n",
      "training loss:   0.025711033893749118 \n",
      "at epoch 52: \n",
      "validation loss: 0.6919342653548464 \n",
      "training loss:   0.019767251543235033 \n",
      "at epoch 53: \n",
      "validation loss: 0.6729746980870024 \n",
      "training loss:   0.018747113302815706 \n",
      "at epoch 54: \n",
      "validation loss: 0.6972315863091895 \n",
      "training loss:   0.015381702714366839 \n",
      "at epoch 55: \n",
      "validation loss: 0.7034259000991253 \n",
      "training loss:   0.01676078904652968 \n",
      "at epoch 56: \n",
      "validation loss: 0.6871781361863968 \n",
      "training loss:   0.021639804844744502 \n",
      "at epoch 57: \n",
      "validation loss: 0.7434912114701373 \n",
      "training loss:   0.030929877297021447 \n",
      "at epoch 58: \n",
      "validation loss: 0.7232999915772296 \n",
      "training loss:   0.02702865479979664 \n",
      "at epoch 59: \n",
      "validation loss: 0.6985699477347922 \n",
      "training loss:   0.02130748393945396 \n",
      "at epoch 60: \n",
      "validation loss: 0.7418430117850608 \n",
      "training loss:   0.015171277248300611 \n",
      "at epoch 61: \n",
      "validation loss: 0.7326617684770138 \n",
      "training loss:   0.019815149563364685 \n",
      "at epoch 62: \n",
      "validation loss: 0.7369821635966606 \n",
      "training loss:   0.013915321892127394 \n",
      "at epoch 63: \n",
      "validation loss: 0.7314097716453227 \n",
      "training loss:   0.01817597884917632 \n",
      "at epoch 64: \n",
      "validation loss: 0.7115339744598308 \n",
      "training loss:   0.02399924388155341 \n",
      "at epoch 65: \n",
      "validation loss: 0.7387577720144962 \n",
      "training loss:   0.013716001962311566 \n",
      "at epoch 66: \n",
      "validation loss: 0.7214835108594692 \n",
      "training loss:   0.009589592854026704 \n",
      "at epoch 67: \n",
      "validation loss: 0.7299995555522594 \n",
      "training loss:   0.013860069301445037 \n",
      "at epoch 68: \n",
      "validation loss: 0.7231245459394252 \n",
      "training loss:   0.03144113885471597 \n",
      "at epoch 69: \n",
      "validation loss: 0.7602664428822538 \n",
      "training loss:   0.022694046560209246 \n",
      "at epoch 70: \n",
      "validation loss: 0.7430150699108204 \n",
      "training loss:   0.014807669748552143 \n",
      "at epoch 71: \n",
      "validation loss: 0.7542907443452389 \n",
      "training loss:   0.006295253593707457 \n",
      "at epoch 72: \n",
      "validation loss: 0.7887975529153296 \n",
      "training loss:   0.004115038475138135 \n",
      "at epoch 73: \n",
      "validation loss: 0.7685318747733502 \n",
      "training loss:   0.010554297604830936 \n",
      "at epoch 74: \n",
      "validation loss: 0.762273593785915 \n",
      "training loss:   0.03142907024128363 \n",
      "at epoch 75: \n",
      "validation loss: 0.7216552977866315 \n",
      "training loss:   0.031647197152487934 \n",
      "at epoch 76: \n",
      "validation loss: 0.74808968445088 \n",
      "training loss:   0.01721570797264576 \n",
      "at epoch 77: \n",
      "validation loss: 0.7525571242291876 \n",
      "training loss:   0.007639047139091418 \n",
      "at epoch 78: \n",
      "validation loss: 0.7815141069128159 \n",
      "training loss:   0.005668830948416144 \n",
      "at epoch 79: \n",
      "validation loss: 0.7999118921604562 \n",
      "training loss:   0.0046214321651496 \n",
      "at epoch 80: \n",
      "validation loss: 0.8057115826200931 \n",
      "training loss:   0.004464223215472884 \n",
      "at epoch 81: \n",
      "validation loss: 0.7822114920362513 \n",
      "training loss:   0.016193070365116 \n",
      "at epoch 82: \n",
      "validation loss: 0.7692623683746825 \n",
      "training loss:   0.04087826472707093 \n",
      "at epoch 83: \n",
      "validation loss: 0.7638144936967404 \n",
      "training loss:   0.019839619847480207 \n",
      "at epoch 84: \n",
      "validation loss: 0.7585108400659358 \n",
      "training loss:   0.011124884249875322 \n",
      "at epoch 85: \n",
      "validation loss: 0.7688888431863582 \n",
      "training loss:   0.005710856737568975 \n",
      "at epoch 86: \n",
      "validation loss: 0.7840862952648325 \n",
      "training loss:   0.007232199370046146 \n",
      "at epoch 87: \n",
      "validation loss: 0.8340254299184109 \n",
      "training loss:   0.005817597462446429 \n",
      "at epoch 88: \n",
      "validation loss: 0.8329938237971448 \n",
      "training loss:   0.008891417171689682 \n",
      "at epoch 89: \n",
      "validation loss: 0.7684341605673445 \n",
      "training loss:   0.05215528409229592 \n",
      "at epoch 90: \n",
      "validation loss: 0.7526883641456036 \n",
      "training loss:   0.023819755756994708 \n",
      "at epoch 91: \n",
      "validation loss: 0.7709324011143218 \n",
      "training loss:   0.009309230278013274 \n",
      "at epoch 92: \n",
      "validation loss: 0.7762528188685154 \n",
      "training loss:   0.005726797563838772 \n",
      "at epoch 93: \n",
      "validation loss: 0.7893702749242174 \n",
      "training loss:   0.0020201781162177213 \n",
      "at epoch 94: \n",
      "validation loss: 0.7971151921343296 \n",
      "training loss:   0.0012588860643154476 \n",
      "at epoch 95: \n",
      "validation loss: 0.8094248917508633 \n",
      "training loss:   0.0008341176489193458 \n",
      "at epoch 96: \n",
      "validation loss: 0.8428391941050266 \n",
      "training loss:   0.0005792427215783391 \n",
      "at epoch 97: \n",
      "validation loss: 0.7932204654876222 \n",
      "training loss:   0.022371441717841663 \n",
      "at epoch 98: \n",
      "validation loss: 0.706965889702452 \n",
      "training loss:   0.0607583250105381 \n",
      "at epoch 99: \n",
      "validation loss: 0.7264633273824732 \n",
      "training loss:   0.015783037554938345 \n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "learning_rate= 1e-3\n",
    "epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = Lenet5(batch_norm=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "train(model = model,\n",
    "        val_loader=val_loader,\n",
    "      train_loader = train_loader,\n",
    "      criterion = criterion,\n",
    "      optimizer = optimizer,\n",
    "      epochs = epochs,\n",
    "      session = 'with batch norm')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-24T15:55:07.024686Z",
     "end_time": "2024-06-24T16:23:56.619400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# training the model with weight decay"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learning_rate= 1e-3\n",
    "epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = Lenet5(batch_norm=False)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "\n",
    "\n",
    "train(model = model,\n",
    "        val_loader=val_loader,\n",
    "      train_loader = train_loader,\n",
    "      criterion = criterion,\n",
    "      optimizer = optimizer,\n",
    "      epochs = epochs,\n",
    "      session = 'with weight decay')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## training the model with weight decay $e^{-3}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learning_rate= 1e-3\n",
    "epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = Lenet5()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "\n",
    "\n",
    "train(model = model,\n",
    "        val_loader=val_loader,\n",
    "      train_loader = train_loader,\n",
    "      criterion = criterion,\n",
    "      optimizer = optimizer,\n",
    "      epochs = epochs,\n",
    "      session = 'with weight decay e-3')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training with dropout:\n",
    "Dropout probabilities typically fall in the range of 0.2 to 0.5.\n",
    "Lower values (e.g., 0.2) encourage the model to retain more information during training.\n",
    "Higher values (e.g., 0.5) increase regularization.\n",
    "\n",
    "We decided to choose 0.3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nadav\\DataspellProjects\\Lenet5\\runs/with dropout\n",
      "at epoch 0: \n",
      "validation loss: 0.6282989287630041 \n",
      "training loss:   1.2116805267333985 \n",
      "at epoch 1: \n",
      "validation loss: 0.5418620001762471 \n",
      "training loss:   0.6293291884660721 \n",
      "at epoch 2: \n",
      "validation loss: 0.48962982474489414 \n",
      "training loss:   0.5627479338645935 \n",
      "at epoch 3: \n",
      "validation loss: 0.4585777457724226 \n",
      "training loss:   0.5175037497282028 \n",
      "at epoch 4: \n",
      "validation loss: 0.4297306125468396 \n",
      "training loss:   0.48884334057569506 \n",
      "at epoch 5: \n",
      "validation loss: 0.42248422097652516 \n",
      "training loss:   0.46345547527074815 \n",
      "at epoch 6: \n",
      "validation loss: 0.39867198213617855 \n",
      "training loss:   0.4501953104138374 \n",
      "at epoch 7: \n",
      "validation loss: 0.3879245792297607 \n",
      "training loss:   0.43512858897447587 \n",
      "at epoch 8: \n",
      "validation loss: 0.380385234000835 \n",
      "training loss:   0.42254609942436216 \n",
      "at epoch 9: \n",
      "validation loss: 0.37012468373521845 \n",
      "training loss:   0.41347339272499084 \n",
      "at epoch 10: \n",
      "validation loss: 0.368410939231832 \n",
      "training loss:   0.40161088794469835 \n",
      "at epoch 11: \n",
      "validation loss: 0.3563807448174091 \n",
      "training loss:   0.3838623684644699 \n",
      "at epoch 12: \n",
      "validation loss: 0.35109146510032896 \n",
      "training loss:   0.38718088626861574 \n",
      "at epoch 13: \n",
      "validation loss: 0.34588561222908343 \n",
      "training loss:   0.37691107466816903 \n",
      "at epoch 14: \n",
      "validation loss: 0.3408723105775549 \n",
      "training loss:   0.3699075397849083 \n",
      "at epoch 15: \n",
      "validation loss: 0.33927581348317737 \n",
      "training loss:   0.3730889275670052 \n",
      "at epoch 16: \n",
      "validation loss: 0.33834682404994965 \n",
      "training loss:   0.3632704827189446 \n",
      "at epoch 17: \n",
      "validation loss: 0.33318451268875854 \n",
      "training loss:   0.35859745502471924 \n",
      "at epoch 18: \n",
      "validation loss: 0.3301335161036633 \n",
      "training loss:   0.35344896987080576 \n",
      "at epoch 19: \n",
      "validation loss: 0.3246668022363744 \n",
      "training loss:   0.35353540286421775 \n",
      "at epoch 20: \n",
      "validation loss: 0.3274031363903208 \n",
      "training loss:   0.34552584648132323 \n",
      "at epoch 21: \n",
      "validation loss: 0.3238393436086939 \n",
      "training loss:   0.3489076718688011 \n",
      "at epoch 22: \n",
      "validation loss: 0.3201684013326117 \n",
      "training loss:   0.3399142876267433 \n",
      "at epoch 23: \n",
      "validation loss: 0.3145566424156757 \n",
      "training loss:   0.33380197420716284 \n",
      "at epoch 24: \n",
      "validation loss: 0.316527876131078 \n",
      "training loss:   0.3289401207864284 \n",
      "at epoch 25: \n",
      "validation loss: 0.31844988782355127 \n",
      "training loss:   0.33173285514116285 \n",
      "at epoch 26: \n",
      "validation loss: 0.3138491517685829 \n",
      "training loss:   0.32315670266747476 \n",
      "at epoch 27: \n",
      "validation loss: 0.3093131349441853 \n",
      "training loss:   0.3255561847984791 \n",
      "at epoch 28: \n",
      "validation loss: 0.3056419773938808 \n",
      "training loss:   0.3251002378761768 \n",
      "at epoch 29: \n",
      "validation loss: 0.30478503475797936 \n",
      "training loss:   0.32194664016366004 \n",
      "at epoch 30: \n",
      "validation loss: 0.30850541338007503 \n",
      "training loss:   0.32000410482287406 \n",
      "at epoch 31: \n",
      "validation loss: 0.30662044312091585 \n",
      "training loss:   0.3143756344914436 \n",
      "at epoch 32: \n",
      "validation loss: 0.2985326127803072 \n",
      "training loss:   0.3086911112070084 \n",
      "at epoch 33: \n",
      "validation loss: 0.2996936925548188 \n",
      "training loss:   0.3156383137404919 \n",
      "at epoch 34: \n",
      "validation loss: 0.2972367501005213 \n",
      "training loss:   0.3046746730804443 \n",
      "at epoch 35: \n",
      "validation loss: 0.300003301590047 \n",
      "training loss:   0.3094792006909847 \n",
      "at epoch 36: \n",
      "validation loss: 0.2974811259736406 \n",
      "training loss:   0.3126252645254135 \n",
      "at epoch 37: \n",
      "validation loss: 0.30017105220480167 \n",
      "training loss:   0.30799918830394746 \n",
      "at epoch 38: \n",
      "validation loss: 0.2913644291619037 \n",
      "training loss:   0.3024462951719761 \n",
      "at epoch 39: \n",
      "validation loss: 0.29627502471842665 \n",
      "training loss:   0.2986932349205017 \n",
      "at epoch 40: \n",
      "validation loss: 0.29477868625458253 \n",
      "training loss:   0.2978112421929836 \n",
      "at epoch 41: \n",
      "validation loss: 0.28890421796352306 \n",
      "training loss:   0.30170190513134004 \n",
      "at epoch 42: \n",
      "validation loss: 0.28902976500227096 \n",
      "training loss:   0.2981251947581768 \n",
      "at epoch 43: \n",
      "validation loss: 0.28941229778401395 \n",
      "training loss:   0.29945935249328615 \n",
      "at epoch 44: \n",
      "validation loss: 0.2882317688236845 \n",
      "training loss:   0.28852815940976145 \n",
      "at epoch 45: \n",
      "validation loss: 0.2891664092845105 \n",
      "training loss:   0.2960760807991028 \n",
      "at epoch 46: \n",
      "validation loss: 0.28678394124862994 \n",
      "training loss:   0.29428515926003457 \n",
      "at epoch 47: \n",
      "validation loss: 0.2872533233875924 \n",
      "training loss:   0.2945924384891987 \n",
      "at epoch 48: \n",
      "validation loss: 0.2898537148820593 \n",
      "training loss:   0.29259767398238185 \n",
      "at epoch 49: \n",
      "validation loss: 0.28537581512268556 \n",
      "training loss:   0.29213958635926246 \n",
      "at epoch 50: \n",
      "validation loss: 0.2888163721307795 \n",
      "training loss:   0.28796211048960685 \n",
      "at epoch 51: \n",
      "validation loss: 0.28237077625508006 \n",
      "training loss:   0.2856204876303673 \n",
      "at epoch 52: \n",
      "validation loss: 0.28700126707553864 \n",
      "training loss:   0.2795958706736565 \n",
      "at epoch 53: \n",
      "validation loss: 0.286618635058403 \n",
      "training loss:   0.2831184159219265 \n",
      "at epoch 54: \n",
      "validation loss: 0.28677244357606196 \n",
      "training loss:   0.28985303685069086 \n",
      "at epoch 55: \n",
      "validation loss: 0.2819977802165011 \n",
      "training loss:   0.28038366585969926 \n",
      "at epoch 56: \n",
      "validation loss: 0.2850613032883786 \n",
      "training loss:   0.27865283817052844 \n",
      "at epoch 57: \n",
      "validation loss: 0.2840764110392712 \n",
      "training loss:   0.2821753707528114 \n",
      "at epoch 58: \n",
      "validation loss: 0.27939010395648634 \n",
      "training loss:   0.2845613652467728 \n",
      "at epoch 59: \n",
      "validation loss: 0.2807283810478576 \n",
      "training loss:   0.28073475152254107 \n",
      "at epoch 60: \n",
      "validation loss: 0.2762393849961301 \n",
      "training loss:   0.28031169816851614 \n",
      "at epoch 61: \n",
      "validation loss: 0.27832336882327463 \n",
      "training loss:   0.2748596307635307 \n",
      "at epoch 62: \n",
      "validation loss: 0.27859448403754133 \n",
      "training loss:   0.28123019471764565 \n",
      "at epoch 63: \n",
      "validation loss: 0.2792024365130891 \n",
      "training loss:   0.2810665173828602 \n",
      "at epoch 64: \n",
      "validation loss: 0.2778541303061424 \n",
      "training loss:   0.2730215115845203 \n",
      "at epoch 65: \n",
      "validation loss: 0.2788089120641668 \n",
      "training loss:   0.273135871887207 \n",
      "at epoch 66: \n",
      "validation loss: 0.2798608750738996 \n",
      "training loss:   0.2813575413823128 \n",
      "at epoch 67: \n",
      "validation loss: 0.2783708756274365 \n",
      "training loss:   0.27634136125445363 \n",
      "at epoch 68: \n",
      "validation loss: 0.2787011528902866 \n",
      "training loss:   0.2761298333108425 \n",
      "at epoch 69: \n",
      "validation loss: 0.27681816575375007 \n",
      "training loss:   0.27235680520534516 \n",
      "at epoch 70: \n",
      "validation loss: 0.275404211371503 \n",
      "training loss:   0.2720995019376278 \n",
      "at epoch 71: \n",
      "validation loss: 0.27940945390691146 \n",
      "training loss:   0.27131599470973017 \n",
      "at epoch 72: \n",
      "validation loss: 0.2779809372856262 \n",
      "training loss:   0.272260489910841 \n",
      "at epoch 73: \n",
      "validation loss: 0.2748201907949245 \n",
      "training loss:   0.2664548122882843 \n",
      "at epoch 74: \n",
      "validation loss: 0.27397887757483946 \n",
      "training loss:   0.2660026629269123 \n",
      "at epoch 75: \n",
      "validation loss: 0.28039069568857233 \n",
      "training loss:   0.2669109778106213 \n",
      "at epoch 76: \n",
      "validation loss: 0.2748957780447412 \n",
      "training loss:   0.2595382311940193 \n",
      "at epoch 77: \n",
      "validation loss: 0.27361147930013374 \n",
      "training loss:   0.2665267339348793 \n",
      "at epoch 78: \n",
      "validation loss: 0.2747731861915994 \n",
      "training loss:   0.2648008419573307 \n",
      "at epoch 79: \n",
      "validation loss: 0.2773293738669537 \n",
      "training loss:   0.2634502711892128 \n",
      "at epoch 80: \n",
      "validation loss: 0.273289274028007 \n",
      "training loss:   0.2549809540808201 \n",
      "at epoch 81: \n",
      "validation loss: 0.27093220200944457 \n",
      "training loss:   0.25675340607762337 \n",
      "at epoch 82: \n",
      "validation loss: 0.27742159144675477 \n",
      "training loss:   0.2649960987269878 \n",
      "at epoch 83: \n",
      "validation loss: 0.2714663972245886 \n",
      "training loss:   0.26370221585035325 \n",
      "at epoch 84: \n",
      "validation loss: 0.27256222639946226 \n",
      "training loss:   0.26137890383601187 \n",
      "at epoch 85: \n",
      "validation loss: 0.26855360320273863 \n",
      "training loss:   0.25539324909448624 \n",
      "at epoch 86: \n",
      "validation loss: 0.2694329039213505 \n",
      "training loss:   0.25973980128765106 \n",
      "at epoch 87: \n",
      "validation loss: 0.2715008534649585 \n",
      "training loss:   0.25634254693984987 \n",
      "at epoch 88: \n",
      "validation loss: 0.27204720961286666 \n",
      "training loss:   0.25034846618771556 \n",
      "at epoch 89: \n",
      "validation loss: 0.2721036818433315 \n",
      "training loss:   0.2529510834813118 \n",
      "at epoch 90: \n",
      "validation loss: 0.26816540354109825 \n",
      "training loss:   0.2533832819759846 \n",
      "at epoch 91: \n",
      "validation loss: 0.2715422165520648 \n",
      "training loss:   0.2615638817846775 \n",
      "at epoch 92: \n",
      "validation loss: 0.2709993387156345 \n",
      "training loss:   0.2540365353226662 \n",
      "at epoch 93: \n",
      "validation loss: 0.2695501297712326 \n",
      "training loss:   0.254645072221756 \n",
      "at epoch 94: \n",
      "validation loss: 0.2703266917391026 \n",
      "training loss:   0.25372080758214 \n",
      "at epoch 95: \n",
      "validation loss: 0.2665736783058085 \n",
      "training loss:   0.2553549826145172 \n",
      "at epoch 96: \n",
      "validation loss: 0.271003949832409 \n",
      "training loss:   0.24878488302230836 \n",
      "at epoch 97: \n",
      "validation loss: 0.2707212532454349 \n",
      "training loss:   0.2524117946624756 \n",
      "at epoch 98: \n",
      "validation loss: 0.26841191567004996 \n",
      "training loss:   0.2524995917081833 \n",
      "at epoch 99: \n",
      "validation loss: 0.2714088926924036 \n",
      "training loss:   0.2510941921174526 \n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "learning_rate= 1e-3\n",
    "epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "dropout_prob = 0.3\n",
    "\n",
    "model = Lenet5(dropout_prob=dropout_prob)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "train(model = model,\n",
    "      val_loader=val_loader,\n",
    "      train_loader = train_loader,\n",
    "      criterion = criterion,\n",
    "      optimizer = optimizer,\n",
    "      epochs = epochs,\n",
    "      session = 'with dropout')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-24T17:44:05.413077Z",
     "end_time": "2024-06-24T18:14:13.745102Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# testing the model\n",
    "\n",
    "imgs, labels = next(iter(test_loader))\n",
    "outputs = model(imgs)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# visualize the images\n",
    "fig, ax = plt.subplots(1, 5, figsize=(20, 5))\n",
    "for i in range(5):\n",
    "    ax[i].imshow(imgs[i].squeeze(), cmap='gray')\n",
    "    ax[i].set_title(f'predicted: {fashion_mnist_train.classes[predicted[i]]}, actual: {fashion_mnist_train.classes[labels[i]]}')\n",
    "    ax[i].axis('off')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate your model\n",
    "now for the exciting part! let's evaluate the model on the test set and see how well it performs on the train set, unseen by it until now.\n",
    "\n",
    "IMPORTANT: double-check the path to the models directory is the correct path ON YOUR MACHINE before running the code below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============\n",
      "\n",
      "Lenet5_with_batch_norm.pth\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[63], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m model \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./models/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m#model.load_state_dict(torch.load(f'./models/{model_path}'))\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval\u001B[49m()\n\u001B[0;32m     13\u001B[0m accuracy, loss \u001B[38;5;241m=\u001B[39m evaluate(model, test_loader)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124maccuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "#edit according to the path to your models directory!!!\n",
    "PATH_TO_MODELS_DIR = '/models/'\n",
    "\n",
    "#for each file in the models directory\n",
    "for model_path in os.listdir(PATH_TO_MODELS_DIR):\n",
    "    print(f'============\\n\\n{model_path}')\n",
    "\n",
    "    model = torch.load(f'{PATH_TO_MODELS_DIR}{model_path}')\n",
    "    #model.load_state_dict(torch.load(f'./models/{model_path}'))\n",
    "    model.eval()\n",
    "    accuracy, loss = evaluate(model, test_loader)\n",
    "    print(f'model {model_path}:\\n\\naccuracy: {accuracy}, loss: {loss}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-14T17:11:45.407826Z",
     "end_time": "2024-06-14T18:10:17.611399Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
