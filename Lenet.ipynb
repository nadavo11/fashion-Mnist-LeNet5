{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T22:06:18.336609Z",
     "start_time": "2024-06-13T22:06:14.916183Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1\n",
    "\n",
    "! pip install tensorboard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Lenet-5 for fashion mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T21:36:46.173924Z",
     "start_time": "2024-06-13T21:36:46.164735Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# fasion-MNIST dataset\n",
    "\n",
    "the fasion-MNIST dataset is a dataset of Zalando's article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends fasion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n",
    "\n",
    "## loading the dataset\n",
    "The dataset can be used from `torchvision.datasets` package. The dataset is downloaded from the internet and saved in the `data` directory. The dataset is loaded using the `torch.utils.data.DataLoader` class. The `DataLoader` class is used to load the data in batches. The `DataLoader` class takes the dataset and the batch size as input and provides an iterator to iterate over the dataset in batches.\n",
    "\n",
    "\n",
    "## Transforms\n",
    "### resize:\n",
    " The images in the fasion-MNIST dataset are of size 28x28. So, it makes sense to use the `transforms.Resize` to convert the images to a size that is compatible with the LeNet-5 architecture's input. The LeNet-5 architecture expects images of size 32x32 as input.\n",
    "the original size of the images in the MNIST dataset is 28x28.\n",
    "\n",
    "### ToTensor:\n",
    "The `transforms.ToTensor` converts the images to PyTorch tensors. The images in the fasion-MNIST dataset are of type `PIL.Image.Image`. The `transforms.ToTensor` converts the images to PyTorch tensors of shape `(C, H, W)` where `C` is the number of channels, `H` is the height of the image, and `W` is the width of the image. The images in the fasion-MNIST dataset are grayscale images, so the number of channels is 1. The shape of the images after applying the `transforms.ToTensor` is `(1, 32, 32)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T22:38:01.496538Z",
     "start_time": "2024-06-13T22:38:01.392716Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fashion_mnist_train = datasets.FashionMNIST('data', train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor()\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T22:38:03.120186Z",
     "start_time": "2024-06-13T22:38:03.078155Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fashion_mnist_test = datasets.FashionMNIST('data', train=False, download=True, transform=transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor()\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### split the dataset to test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T22:38:04.042871Z",
     "start_time": "2024-06-13T22:38:03.991331Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split fashion_mnist_train into train and val sets\n",
    "train_indices, val_indices = train_test_split(list(range(len(fashion_mnist_train))), test_size=0.2, random_state=42)\n",
    "\n",
    "train_subset = Subset(fashion_mnist_train, train_indices)\n",
    "val_subset = Subset(fashion_mnist_train, val_indices)\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(fashion_mnist_test, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tensorboard\n",
    "\n",
    "tensorboard is a visualization tool that can be used to visualize the training process of a deep learning model. The `torch.utils.tensorboard.SummaryWriter` class is used to write the logs to the tensorboard. The `SummaryWriter` class takes the log directory as input. The logs are written to the log directory in the form of event files. The event files can be visualized using the tensorboard web interface.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T10:27:46.975621Z",
     "start_time": "2024-06-04T10:27:46.888530Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%tensorboard --logdir runs\n",
    "log_dir = os.path.join(os.getcwd(), \"runs\")\n",
    "os.makedirs(log_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Visualize the dataset\n",
    "let's draw 5 random images from the dataset and visualize them. The images are grayscale images of size 32x32. The images are converted to PyTorch tensors of shape `(1, 32, 32)` using the `transforms.ToTensor` transform. The images are then visualized using the `matplotlib.pyplot.imshow` function. we can also decode the labels to see how the images are labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T22:38:08.424346Z",
     "start_time": "2024-06-13T22:38:07.667458Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get 5 images\n",
    "imgs = [0]*5\n",
    "labels = [0]*5\n",
    "for idx, i in enumerate(np.random.randint(0, len(fashion_mnist_train), 5)):\n",
    "    imgs[idx], labels[idx] = fashion_mnist_train[i]\n",
    "\n",
    "#visualize the 5 image\n",
    "fig, ax = plt.subplots(1, 5, figsize=(20, 5))\n",
    "for i in range(5):\n",
    "    ax[i].imshow(imgs[i].permute(1, 2, 0), cmap='gray')\n",
    "    ax[i].set_title(fashion_mnist_train.classes[labels[i]])\n",
    "    ax[i].axis('off')\n",
    "    #decode the label\n",
    "    label = fashion_mnist_train.classes[labels[i]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "markdown bold text -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# LeNet-5 Architecture\n",
    "The LeNet-5 architecture is a convolutional neural network architecture proposed by Yann LeCun in 1998. The LeNet-5 architecture consists of 7 layers. The architecture of the LeNet-5 is as follows:\n",
    "\n",
    "#### **input layer:**\n",
    "The input to the LeNet-5 architecture is a grayscale image of size 32x32.\n",
    "\n",
    "#### Convolution & Average Pooling Layers:\n",
    "next, the input image is passed through a convolutional layer followed by an average pooling layer.\n",
    "The convolutional layer has 6 filters of size 5x5.\n",
    "- **kernel size:** 5x5\n",
    "-  **channels:** $1 \\rightarrow 6$\n",
    "- **stride:** 1\n",
    "The average pooling layer has a filter of size 2x2.\n",
    "- **kernel size:** 2x2\n",
    "- **stride:** 2\n",
    "\n",
    "#### 2nd Convolution & Average Pooling Layers:\n",
    "The output from the first average pooling layer is passed through another convolutional layer followed by another average pooling layer.\n",
    "The convolutional layer has 16 filters of size 5x5.\n",
    "- **kernel size:** 5x5\n",
    "- **channels:** $6 \\rightarrow 16$\n",
    "- **stride:** 1\n",
    "average pooling layer has a filter of size 2x2.\n",
    "- **kernel size:** 2x2\n",
    "- **stride:** 2\n",
    "\n",
    "#### convolutional layer:\n",
    "The output from the second average pooling layer is passed through another convolutional layer.\n",
    "- **kernel size:** 5x5\n",
    "- **output channels:** 120\n",
    "- **stride:** 1\n",
    "\n",
    "#### Fully Connected Layers:\n",
    "The output from the last convolutional layer is flattened and passed through 3 fully connected layers:\n",
    "- **1st fully connected layer:** $120 \\rightarrow 84$\n",
    "- **2nd fully connected layer:** $84 \\rightarrow 10$\n",
    "#### Output Layer:\n",
    "The output from the last fully connected layer is passed through a softmax activation function to get the best of 10 class probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T22:38:10.413766Z",
     "start_time": "2024-06-13T22:38:10.408259Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:15:50.624055Z",
     "start_time": "2024-06-14T14:15:50.567496Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Lenet5(nn.Module):\n",
    "    def __init__(self, dropout_prob=0, batch_norm=False):\n",
    "        super(Lenet5, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm2d(6) if batch_norm else nn.Identity()\n",
    "        self.batch_norm2 = nn.BatchNorm2d(16) if batch_norm else nn.Identity()\n",
    "        self.batch_norm3 = nn.BatchNorm2d(120) if batch_norm else nn.Identity()\n",
    "        self.batch_norm_fc1 = nn.BatchNorm1d(84) if batch_norm else nn.Identity()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Tanh(),\n",
    "            self.batch_norm1,\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Tanh(),\n",
    "            self.batch_norm2,\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 120, kernel_size=5, stride=1),\n",
    "            nn.Tanh(),\n",
    "            self.batch_norm3,\n",
    "        )\n",
    "\n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Tanh(),\n",
    "            self.batch_norm_fc1,)\n",
    "\n",
    "        self.fc2 = nn.Linear(84, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        self.dropout\n",
    "        x = self.conv3(x)\n",
    "        self.dropout\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        #x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "let's initialize the model and print the model summary.then let's try it on a single batch of images to see if the model is working correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T22:39:37.671849Z",
     "start_time": "2024-06-13T22:39:37.623834Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Lenet5()\n",
    "print(model)\n",
    "\n",
    "pred = model(fashion_mnist_train[0][0].unsqueeze(0))\n",
    "print (pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## visualizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T22:39:40.091329Z",
     "start_time": "2024-06-13T22:39:39.391251Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tb_writer = SummaryWriter('runs/lenet5')\n",
    "\n",
    "# visualize the model in our tensorboard summary\n",
    "tb_writer.add_graph(model, fashion_mnist_train[0][0].unsqueeze(0))\n",
    "tb_writer.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T22:39:42.522404Z",
     "start_time": "2024-06-13T22:39:42.491412Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 256\n",
    "dropout_prob = 0.3\n",
    "weight_decay = 0.0001\n",
    "batch_norm = False\n",
    "learning_rate= 1e-3\n",
    "epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# training the model\n",
    "train_loader = DataLoader(fashion_mnist_train, batch_size=batch_size, shuffle=True)\n",
    "#split the dataset to test and validation\n",
    "test_loader = DataLoader(fashion_mnist_test, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(fashion_mnist_train, batch_size=batch_size, shuffle=False)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T22:39:44.238898Z",
     "start_time": "2024-06-13T22:39:44.208870Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    \"\"\"\n",
    "    evaluate the model on the validation set\n",
    "    :param model: model to evaluate\n",
    "    :param val_loader: validation dataset loader\n",
    "    :return: (accuracy, loss)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            running_acc += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        accuracy = running_acc / total\n",
    "        loss = running_loss/ len(val_loader)\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T17:06:45.171638Z",
     "start_time": "2024-06-14T17:06:45.128051Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def train(model = Lenet5,\n",
    "          train_loader=train_loader,\n",
    "          criterion = criterion,\n",
    "          optimizer = optimizer,\n",
    "          epochs=epochs,\n",
    "          session = None,\n",
    "          batch_norm = False,\n",
    "          dropout = 0.2,\n",
    "          ):\n",
    "\n",
    "    # track with tensorboard\n",
    "    session = session or 'Lenet5'+datetime.now().strftime('%m-%d-%H-%M')\n",
    "    #tb_writer = SummaryWriter(f'runs/{session}')\n",
    "    run_dir = f'{log_dir}/{session}'\n",
    "    print(run_dir)\n",
    "    tb_writer = SummaryWriter(run_dir)\n",
    "    tb_writer.flush()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        train_total = 0\n",
    "        val_loss_min = 0\n",
    "        # set the model to train mode\n",
    "        model.train(True)\n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "            # get the input image and labels\n",
    "            inputs, labels = data\n",
    "\n",
    "            # start with zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            outputs = model(inputs)\n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            if i % 100 == 99:\n",
    "                # print gradient statistics\n",
    "                for name, param in model.named_parameters():\n",
    "                    tb_writer.add_histogram(name, param.grad, epoch * len(train_loader) +  i)\n",
    "                # loss of current batch\n",
    "                avg_train_loss = running_loss / 100\n",
    "                tb_writer.add_scalar('training loss', avg_train_loss, epoch * len(train_loader) +  i)\n",
    "\n",
    "                #print(f'[{epoch + 1}, {i + 1}] loss: {avg_train_loss}')\n",
    "                running_loss = 0.0\n",
    "        \"\"\"-----------------\n",
    "        per epoch evaluation\n",
    "        -----------------\"\"\"\n",
    "        # set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        #accuracy\n",
    "        train_accuracy = running_corrects / train_total\n",
    "        # validation\n",
    "        val_acc, val_loss = evaluate(model, val_loader)\n",
    "\n",
    "        #val_accuracy = (val_preds == val_labels).sum().item() / len(val_loader)\n",
    "        tb_writer.add_scalars('train vs val loss', {'train': avg_train_loss, 'val': val_loss}, epoch)\n",
    "        tb_writer.add_scalars('train vs val accuracy', {'train': train_accuracy, 'val': val_acc}, epoch)\n",
    "        print(f'at epoch {epoch}: \\nvalidation loss: {val_loss} \\ntraining loss:   {avg_train_loss} ')\n",
    "        tb_writer.add_scalar('validation loss', val_loss, epoch)\n",
    "        if val_loss <= val_loss_min:\n",
    "            print('validation loss decreased({:.6f} -->{:.6f}). Saving Model ...'.format(valid_loss_min, valid_loss))\n",
    "            torch.save(model.state_dict(), f'./models/Lenet_{session}.pth')\n",
    "            valid_loss_min = valid_loss\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# training the model without dropout, batch normalization or weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T17:07:12.039171Z",
     "start_time": "2024-06-14T17:07:11.927108Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate= 1e-3\n",
    "epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = Lenet5(dropout=0, batch_norm=False)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "train(model = model,\n",
    "      train_loader = train_loader,\n",
    "      criterion = criterion,\n",
    "      optimizer = optimizer,\n",
    "      epochs = epochs,\n",
    "      session='vanilla_lenet5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# training the model with batch normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate= 1e-3\n",
    "epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = Lenet5(dropout=0, batch_norm=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "train(model = model,\n",
    "      train_loader = train_loader,\n",
    "      criterion = criterion,\n",
    "      optimizer = optimizer,\n",
    "      epochs = epochs,\n",
    "      session = 'with_batch_norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# training the model with weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate= 1e-3\n",
    "epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = Lenet5(dropout=0, batch_norm=False)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "\n",
    "\n",
    "train(model = model,\n",
    "      train_loader = train_loader,\n",
    "      criterion = criterion,\n",
    "      optimizer = optimizer,\n",
    "      epochs = epochs,\n",
    "      session = 'with_weight_decay')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with dropout:\n",
    "Dropout probabilities typically fall in the range of 0.2 to 0.5.\n",
    "Lower values (e.g., 0.2) encourage the model to retain more information during training.\n",
    "Higher values (e.g., 0.5) increase regularization.\n",
    "\n",
    "We decided to choose 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Lenet5(dropout_prob=dropout_prob, batch_norm=False)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "train(model = model,\n",
    "      train_loader = train_loader,\n",
    "      criterion = criterion,\n",
    "      optimizer = optimizer,\n",
    "      epochs = epochs,\n",
    "      session = 'with_dropout_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T16:33:59.054843Z",
     "start_time": "2024-06-04T16:33:57.943426Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# testing the model\n",
    "\n",
    "imgs, labels = next(iter(test_loader))\n",
    "outputs = model(imgs)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# visualize the images\n",
    "fig, ax = plt.subplots(1, 5, figsize=(20, 5))\n",
    "for i in range(5):\n",
    "    ax[i].imshow(imgs[i].squeeze(), cmap='gray')\n",
    "    ax[i].set_title(f'predicted: {fashion_mnist_train.classes[predicted[i]]}, actual: {fashion_mnist_train.classes[labels[i]]}')\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T18:10:17.611399Z",
     "start_time": "2024-06-14T17:11:45.407826Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
